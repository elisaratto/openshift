# Red Hat OpenShift Container Platform
## OpenShift Container Platform Features  
![Openshift-Architecture](images/openshift-architecture.png)

## Openshift Features
![Openshift-Features](images/openshift-features.png)

## Openshift Control Plane 
![Openshift-Control-Plane](images/openshift-control-plane.png)  
##### this document using Openshift ver.4.5

## Openshift Cluster Operators  
- Kubernetes Operators  
    - Operators usually define custom resources (CR) that store their settings and configurations.  
    - The syntax of a custom resource is defined by a custom resource definition (CRD).  
    - An OpenShift administrator manages an operator by editing its custom resources.  

- Openshift Cluster Operators manages some following list of cluster operator: 
    - network  
    - ingress  
    - storage  
    - authentication  
    - console  
    - monitoring  
    - image-registry  
    - cluster-autoscaler  
    - openshift-apiserver  
    - dns  
    - openshift-controller-manager  
    - cloud-credential  
![Openshift-operator](images/openshift-operator.jpeg)  

- Operator  
    - An application that manages Kubernetes resources.  
- Operator SDK  
    - An open source toolkit for building, testing, and packaging operators.  
- Operator Catalog  
    - A repository for discovering and installing operators.  
- Custom Resource Definition (CRD)  
    - An extension of the Kubernetes API that defines the syntax of a custom resource.  
- Operator Lifecycle Manager (OLM)  
    - An application that manages Kubernetes operators.  
- OperatorHub  
    - A public web service where you can publish operators that are compatible with the OLM.  
- Operator Image  
    - The artifact defined by the Operator Framework that you can publish for consumption by an OLM instance.  


## Troubleshooting OpenShift Clusters and Applications  
### Verifying the Health of OpenShift Nodes  
- `oc get nodes`  
```bash
NAME       STATUS   ROLES           AGE   VERSION
master01   Ready    master,worker   2d    v1.18.3+012b3ec
master02   Ready    master,worker   2d    v1.18.3+012b3ec
master03   Ready    master,worker   2d    v1.18.3+012b3ec
```

- `oc adm top nodes`  
Displays the current CPU and memory usage of each node.  
These are actual usage numbers, not the resource requests that the OpenShift scheduler considers as the available and used capacity of the node.  
```bash
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
master01   499m         14%    3235Mi          21%
master02   769m         21%    4933Mi          33%
master03   1016m        29%    6087Mi          40%
```

- `oc describe node my-node-name`  
To retrieve the cluster version  
```bash
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.5.4     True        False         4d23h   Cluster version is 4.5.4
```

- `oc get clusteroperators`  
```bash
NAME                      VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication            4.5.4     True        False         False      3h58m
cloud-credential          4.5.4     True        False         False      4d23h
cluster-autoscaler        4.5.4     True        False         False      4d23h
config-operator           4.5.4     True        False         False      4d23h
console                   4.5.4     True        False         False      3h58m
csi-snapshot-controller   4.5.4     True        False         False      4d23h
dns                       4.5.4     True        False         False      4d23h
etcd                      4.5.4     True        False         False      4d23h
image-registry            4.5.4     True        False         False      4d23h
...output omitted...
```  

#### Openshift Nodes  
##### Show logs of Openshift Nodes  
- `oc adm node-logs -u crio my-node-name`  
    - display the crio service logs on my-node-name OCP node  
- `oc adm node-logs -u kubelet my-node-name`  
    - display the kubelet service logs on my-node-name OCP node  
- `oc adm node-logs my-node-name`  
    - display all journal logs of node  

##### Open a shell prompt on an Openshift Node  
- `oc debug node/my-node-name`  
```bash
Starting pod/master01-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.50.10
If you don't see a command prompt, try pressing enter.

sh-4.2# chroot /host
sh-4.2# 
sh-4.2# systemctl status kubelet
● kubelet.service - MCO environment configuration
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           └─10-mco-default-env.conf
   Active: active (running) since Fri 2020-07-31 16:26:57 UTC; 4h 32min ago
...output omitted...
sh-4.2# 
sh-4.2# systemctl status cri-o
● crio.service - MCO environment configuration
   Loaded: loaded (/usr/lib/systemd/system/crio.service; disabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/crio.service.d
           └─10-mco-default-env.conf
   Active: active (running) since Fri 2020-07-31 16:26:50 UTC; 4h 35min ago
...output omitted...

```

#### Troubleshooting Application Deployments  
`oc get pod`  
`oc status`  
`oc describe pod my-pod-name`  
`oc logs my-pod-name`  
`oc logs my-pod-name -c container-name`  
`oc logs my-pod-name --all-containers`  

##### Creating troubleshooting pod using `oc debug`  
`oc debug deployment/my-deployment-name --as-root`  

##### rsh command  
`oc rsh my-pod-name`  
    Opens a shell inside a pod to run shell commands interactively and non-interactively.  

##### Copy files  
`oc cp /local/path my-pod-name:/container/path`  

##### Verbose log level  
`oc get pod --loglevel 6`  
`oc get pod --loglevel 10`  

##### Get current token  
`oc whoami -t`  



##### Port-forward  
`oc port-forward my-pod-name local-port:remote-port`  
    Creates a TCP tunnel from local-port on your workstation to local-port on the pod. The tunnel is alive as long as you keep the oc port-forward running. This allows you to get network access to the pod without exposing it through a route. Because the tunnel starts at your localhost, it cannot be accessed by other machines.

##### skopeo inspect  
`skopeo inspect docker://registry.access.redhat.com/rhscl/postgresq-96-rhel7:1`
```bash
FATA[0000] Error parsing image name "docker://registry.access.redhat.com/rhscl/postgresq-96-rhel7:1": Error reading manifest 1 in registry.access.redhat.com/rhscl/postgresq-96-rhel7: name unknown: Repo not found
```

`skopeo inspect docker://registry.access.redhat.com/rhscl/postgresql-96-rhel7:1`  
This command checks whether the remote registry access is exist or not.



## OpenShift Dynamic Storage  
### Persistent Storage  
Two ways of provisioning storage for the cluster:  
- Static  
    - Static provisioning requires the cluster administrator to create persistent volumes manually.  
- Dynamic  
    - Dynamic provisioning uses storage classes to create the persistent volumes on demand.  
    - Verifying dynamic storageclass:  
    ```bash
    oc get storageclass
        NAME                    PROVISIONER               ...
        nfs-storage (default)   nfs-storage-provisioner   ...
    ```  

#### PVC and PV  
- PVC: persistent volume claim  
    - Specify a name for the persistent volume claim.This name is use in the `claimName` field in the volume section of Deployment manifest.  
    - Important to specify `Access Modes`. If PV persistent volumes are created statically, then an eligible persistent volume must provide this access mode.  
        - ReadWriteMany(RWX): Kubernetes can mount the volume as read-write on many nodes.  
        - ReadOnlyMany(ROX): Kubernetes can mount the volume as read-only on many nodes.  
        - ReadWriteOnce(RWO): Kubernetes can mount the volume as read-write on only a single node.  
    - Important to specify `Size Request`.  If persistent volumes are created statically, then an eligible persistent volume must be at least the requested size.  

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example-pv-claim
  labels:
    app: example-application
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 15Gi
```  

##### Add PVC on the application  
```yaml
...
spec:
   volumes:
     - name: example-pv-storage
       persistentVolumeClaim:
         claimName: example-pv-claim
   containers:
   - name: example-application
     image: registry.redhat.io/rhel8/example-app
     ports:
     - containerPort: 1234
     volumeMounts:
       - mountPath: "/var/lib/example-app"
         name: example-pv-storage
```

## # Authentication and Authorization
#### # OpenShift Users and Groups  
`User`  
In the OpenShift Container Platform architecture, users are entities that interact with the API server. The user resource represents an actor within the system. Assign permissions by adding roles to the user directly or to the groups of which the user is a member.  

`Group`  
Groups represent a specific set of users. Users are assigned to one or to multiple groups. Groups are leveraged when implementing authorization policies to assign permissions to multiple users at the same time.  

`Identity`  
The identity resource keeps a record of successful authentication attempts from a specific user and identity provider. Any data concerning the source of the authentication is stored on the identity. Only a single user resource is associated with an identity resource.  

`Service Account`  
In OpenShift, applications can communicate with the API independently when user credentials cannot be acquired. To preserve the integrity of a regular user's credentials, credentials are not shared and service accounts are used instead. Service accounts enable you to control API access without the need to borrow a regular user's credentials.  

`Role`  
A role defines a set of permissions that enables a user to perform API operations over one or more resource types. You grant permissions to users, groups, and service accounts by assigning roles to them.  

#### # Authenticating API Requests  
##### 1. OAuth Access Tokens  
##### 2. X.509 Client Certificates  
The installation logs provide the location of the kubeconfig file:  
`INFO Run 'export KUBECONFIG=root/auth/kubeconfig' to manage the cluster with 'oc'.`  
This kubeconfig file can be used to authenticate to the OCP cluster.  

#### # Authenticating API Requests  
To improve OpenShift cluster security, you can remove the kubeadmin user credentials after you define an identity provider, create a new user, and assign that user the cluster-admin role.  
```bash
$ oc delete secret kubeadmin -n kube-system

```

```yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  indentityProviders:
  - name: my_httpasswd_provider
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd-secret
```


## # QUIZ: Configuring Identity Providers
#### # Follow below instructions:  
1. Create htpasswd file and add `user1` with the password `pass1`. Set the htpasswd file as /tmp/htpasswd.  
2. Add `user2` with the password `pass2`.  
3. Create secret that contains the HTPasswd users file mentioned above. Secret's name is `localusers`.  
4. Update the HTPasswd identity provider for the cluster so that you can authenticate above users mentioned in the htpasswd file. 

#### # Answer
<details>
<summary>Answer</summary>  

1. Create htpasswd file and add `user1` with the password `pass1`. Set the htpasswd file as /tmp/htpasswd.  
```bash
htpasswd -c -B -b /tmp/htpasswd user1 pass1
```  

2. Add `user2` with the password `pass2`.  
```bash
htpasswd -b /tmp/htpasswd user2 pass2
```  

3. Create secret that contains the HTPasswd users file mentioned above. Secret's name is `localusers`.  
```bash
#Login with admin user
oc login -u kubeadmin -p $KUBEADM_PASSWD $OCP_CLUSTER

oc create secret generic localusers --from-file htpasswd=/tmp/htpasswd -n openshift-config
```

</details>



## # AAA
#### # BBB

#### # Template
<details>
<summary>template</summary>
</details>
